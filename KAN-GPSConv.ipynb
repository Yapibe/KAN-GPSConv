{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KAN-GPSConv: Integrating Kolmogorov-Arnold Networks with Graph Positional Signatures\n",
    "\n",
    "This notebook implements a Graph Neural Network that integrates Kolmogorov-Arnold Network (KAN) layers into Graph Positional Signatures (GPS) networks for both node and graph classification tasks.\n",
    "\n",
    "## Setup and Imports\n",
    "\n",
    "First, we'll install the necessary packages and import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torch_geometric wandb\n",
    "!pip install ogb  # For OGB datasets if needed\n",
    "\n",
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from torch_geometric.datasets import Planetoid, TUDataset\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "from torch_geometric.loader import DataLoader\n",
    "import wandb\n",
    "import argparse\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights & Biases Integration\n",
    "\n",
    "Before running experiments, make sure to log in to Weights & Biases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Implementation\n",
    "\n",
    "### KAN Layer\n",
    "\n",
    "The Kolmogorov-Arnold Network (KAN) layer is implemented as a custom PyTorch module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KANLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, num_neurons=10):\n",
    "        super(KANLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.num_neurons = num_neurons\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.weights = nn.Parameter(torch.Tensor(num_neurons, in_features))\n",
    "        self.biases = nn.Parameter(torch.Tensor(num_neurons))\n",
    "        self.output_weights = nn.Parameter(torch.Tensor(out_features, num_neurons))\n",
    "        \n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.weights)\n",
    "        nn.init.xavier_uniform_(self.output_weights)\n",
    "        nn.init.zeros_(self.biases)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Compute the inner products\n",
    "        inner_products = F.linear(x, self.weights, self.biases)\n",
    "        \n",
    "        # Apply activation function (e.g., sine for KAN)\n",
    "        activations = torch.sin(inner_products)\n",
    "        \n",
    "        # Compute output\n",
    "        output = F.linear(activations, self.output_weights)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPS Network\n",
    "\n",
    "The Graph Positional Signature (GPS) Network integrates the KAN layer with graph convolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPSNetwork(nn.Module):\n",
    "    def __init__(self, num_node_features, num_classes, hidden_dim=64, task='node'):\n",
    "        super(GPSNetwork, self).__init__()\n",
    "        self.task = task\n",
    "        \n",
    "        # Define network layers\n",
    "        self.conv1 = GCNConv(num_node_features, hidden_dim)\n",
    "        self.kan = KANLayer(hidden_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Final classification layer\n",
    "        if task == 'graph':\n",
    "            self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "        else:  # node classification\n",
    "            self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        # First graph convolution\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Apply KAN layer\n",
    "        x = self.kan(x)\n",
    "        \n",
    "        # Second graph convolution\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        if self.task == 'graph':\n",
    "            # Global pooling for graph classification\n",
    "            x = global_mean_pool(x, data.batch)\n",
    "        \n",
    "        # Final classification layer\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "\n",
    "These functions handle dataset loading and preprocessing for both node and graph classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(name, root='/content/data', task='node'):\n",
    "    transform = NormalizeFeatures()\n",
    "    \n",
    "    if task == 'node':\n",
    "        if name in ['Cora', 'Citeseer', 'PubMed']:\n",
    "            dataset = Planetoid(root=root, name=name, transform=transform)\n",
    "        else:\n",
    "            raise ValueError(f\"Node classification dataset {name} not recognized\")\n",
    "        \n",
    "        data = dataset[0]\n",
    "        num_classes = dataset.num_classes\n",
    "        return data, num_classes\n",
    "    \n",
    "    elif task == 'graph':\n",
    "        dataset = TUDataset(root=root, name=name, transform=transform)\n",
    "        num_classes = dataset.num_classes\n",
    "        return dataset, num_classes\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Task must be either 'node' or 'graph'\")\n",
    "\n",
    "def split_data(data, val_ratio=0.1, test_ratio=0.1):\n",
    "    # Split node data into train, validation, and test sets\n",
    "    num_nodes = data.num_nodes\n",
    "    indices = torch.randperm(num_nodes)\n",
    "    test_size = int(num_nodes * test_ratio)\n",
    "    val_size = int(num_nodes * val_ratio)\n",
    "    \n",
    "    data.test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    data.test_mask[indices[:test_size]] = True\n",
    "    \n",
    "    data.val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    data.val_mask[indices[test_size:test_size+val_size]] = True\n",
    "    \n",
    "    data.train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    data.train_mask[indices[test_size+val_size:]] = True\n",
    "    \n",
    "    return data\n",
    "\n",
    "def prepare_graph_data(dataset, batch_size=32, split_ratios=[0.8, 0.1, 0.1]):\n",
    "    # Prepare data loaders for graph classification tasks\n",
    "    num_graphs = len(dataset)\n",
    "    num_train = int(num_graphs * split_ratios[0])\n",
    "    num_val = int(num_graphs * split_ratios[1])\n",
    "    num_test = num_graphs - num_train - num_val\n",
    "    \n",
    "    train_dataset = dataset[:num_train]\n",
    "    val_dataset = dataset[num_train:num_train+num_val]\n",
    "    test_dataset = dataset[num_train+num_val:]\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation Functions\n",
    "\n",
    "These functions handle the training and evaluation processes for both node and graph classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_node(model, data, optimizer):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def train_graph(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "def test_node(model, data, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct = pred[mask] == data.y[mask]\n",
    "        acc = int(correct.sum()) / int(mask.sum())\n",
    "    return acc\n",
    "\n",
    "def test_graph(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(data)\n",
    "            pred = out.argmax(dim=1)\n",
    "            correct += int((pred == data.y).sum())\n",
    "    return correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Training Loop\n",
    "\n",
    "This function orchestrates the entire training process, including data loading, model initialization, training, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    wandb.init(project=\"KAN-GPSConv\", config=args)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    if args.task == 'node':\n",
    "        # Node classification task\n",
    "        data, num_classes = load_dataset(args.dataset, task='node')\n",
    "        data = split_data(data)\n",
    "        data = data.to(device)\n",
    "        \n",
    "        wandb.config.update({\n",
    "            \"model_type\": \"KAN-GPS\",\n",
    "            \"dataset\": args.dataset,\n",
    "            \"task\": args.task,\n",
    "            \"num_features\": data.num_features,\n",
    "            \"num_classes\": num_classes,\n",
    "            \"learning_rate\": 0.01,\n",
    "            \"weight_decay\": 5e-4,\n",
    "            \"num_epochs\": 200\n",
    "        })\n",
    "        \n",
    "        model = GPSNetwork(data.num_features, num_classes, task='node').to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "        \n",
    "        for epoch in range(200):\n",
    "            loss = train_node(model, data, optimizer)\n",
    "            train_acc = test_node(model, data, data.train_mask)\n",
    "            val_acc = test_node(model, data, data.val_mask)\n",
    "            test_acc = test_node(model, data, data.test_mask)\n",
    "            \n",
    "            wandb.log({\n",
    "                \"epoch\": epoch,\n",
    "                \"loss\": loss,\n",
    "                \"train_acc\": train_acc,\n",
    "                \"val_acc\": val_acc,\n",
    "                \"test_acc\": test_acc,\n",
    "                \"gpu_memory_allocated\": torch.cuda.memory_allocated(),\n",
    "                \"gpu_memory_cached\": torch.cuda.memory_cached()\n",
    "            })\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train: {train_acc:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}')\n",
    "    \n",
    "    elif args.task == 'graph':\n",
    "        # Graph classification task\n",
    "        dataset, num_classes = load_dataset(args.dataset, task='graph')\n",
    "        train_loader, val_loader, test_loader = prepare_graph_data(dataset)\n",
    "        \n",
    "        wandb.config.update({\n",
    "            \"model_type\": \"KAN-GPS\",\n",
    "            \"dataset\": args.dataset,\n",
    "            \"task\": args.task,\n",
    "            \"num_features\": dataset.num_node_features,\n",
    "            \"num_classes\": num_classes,\n",
    "            \"learning_rate\": 0.01,\n",
    "            \"weight_decay\": 5e-4,\n",
    "            \"num_epochs\": 200,\n",
    "            \"batch_size\": train_loader.batch_size\n",
    "        })\n",
    "        \n",
    "        model = GPSNetwork(dataset.num_node_features, num_classes, task='graph').to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "        \n",
    "        for epoch in range(200):\n",
    "            loss = train_graph(model, train_loader, optimizer, device)\n",
    "            train_acc = test_graph(model, train_loader, device)\n",
    "            val_acc = test_graph(model, val_loader, device)\n",
    "            test_acc = test_graph(model, test_loader, device)\n",
    "            \n",
    "            wandb.log({\n",
    "                \"epoch\": epoch,\n",
    "                \"loss\": loss,\n",
    "                \"train_acc\": train_acc,\n",
    "                \"val_acc\": val_acc,\n",
    "                \"test_acc\": test_acc,\n",
    "                \"gpu_memory_allocated\": torch.cuda.memory_allocated(),\n",
    "                \"gpu_memory_cached\": torch.cuda.memory_cached()\n",
    "            })\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train: {train_acc:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}')\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Task must be either 'node' or 'graph'\")\n",
    "    \n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Experiment\n",
    "\n",
    "To run an experiment, use the following code. Uncomment and modify as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# main(argparse.Namespace(dataset='Cora', task='node'))\n",
    "# main(argparse.Namespace(dataset='MUTAG', task='graph'))\n",
    "\n",
    "# For interactive argument parsing, you can use this:\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='Train GPS Network on node or graph classification datasets')\n",
    "    parser.add_argument('--dataset', type=str, default='Cora', help='Dataset name')\n",
    "    parser.add_argument('--task', type=str, default='node', choices=['node', 'graph'], help='Task type: node or graph classification')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    main(args)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
